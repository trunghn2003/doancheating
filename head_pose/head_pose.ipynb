{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519555c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸ§  Head Pose Estimation Trong Jupyter Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e74c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing InsightFace model (buffalo_l, CPU)...\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "âœ… InsightFace model loaded successfully!\n",
      "\n",
      "ðŸ§  Æ¯á»›c lÆ°á»£ng Head Pose tá»« áº£nh:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8917dd847842d78f6a4dfbaece88e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value=(), accept='image/*', description='Upload áº¢nh'), Button(button_style='success'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage, clear_output\n",
    "import io\n",
    "import insightface\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Khá»Ÿi táº¡o InsightFace model\n",
    "try:\n",
    "    print(\"ðŸ”§ Initializing InsightFace model (buffalo_l, CPU)...\")\n",
    "    model = insightface.app.FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
    "    model.prepare(ctx_id=-1)  # CPU\n",
    "    print(\"âœ… InsightFace model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading InsightFace model: {e}\")\n",
    "    print(\"Please ensure 'insightface' and 'onnxruntime' are installed: `pip install insightface onnxruntime`\")\n",
    "    raise\n",
    "\n",
    "# HÃ m tÃ­nh head pose tá»« landmarks vÃ  váº½ visualization lÃªn áº£nh\n",
    "def estimate_head_pose(landmarks, img, img_size):\n",
    "    \"\"\"\n",
    "    Æ¯á»›c lÆ°á»£ng head pose (pitch, yaw, roll) tá»« landmarks vÃ  váº½ landmarks + axes lÃªn áº£nh.\n",
    "    Input: landmarks (3D, shape 68x3), img (áº£nh gá»‘c), kÃ­ch thÆ°á»›c áº£nh (height, width)\n",
    "    Output: (pitch, yaw, roll) in degrees, vÃ  áº£nh Ä‘Ã£ váº½\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # CÃ¡c Ä‘iá»ƒm 3D model (tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘iá»ƒm chÃ­nh trÃªn khuÃ´n máº·t trung bÃ¬nh)\n",
    "        model_points = np.array([\n",
    "            [0.0, 0.0, 0.0],             # Nose tip (30)\n",
    "            [0.0, -330.0, -65.0],        # Chin (8)\n",
    "            [-225.0, 170.0, -135.0],     # Left eye left corner (36)\n",
    "            [225.0, 170.0, -135.0],      # Right eye right corner (45)\n",
    "            [-150.0, -150.0, -125.0],    # Left Mouth corner (48)\n",
    "            [150.0, -150.0, -125.0]      # Right mouth corner (54)\n",
    "        ], dtype=\"double\")\n",
    "\n",
    "        # CÃ¡c Ä‘iá»ƒm 2D tÆ°Æ¡ng á»©ng tá»« landmarks 3D (láº¥y x,y)\n",
    "        image_points = np.array([\n",
    "            landmarks[30][:2],  # Nose tip\n",
    "            landmarks[8][:2],   # Chin\n",
    "            landmarks[36][:2],  # Left eye left corner\n",
    "            landmarks[45][:2],  # Right eye right corner\n",
    "            landmarks[48][:2],  # Left mouth corner\n",
    "            landmarks[54][:2]   # Right mouth corner\n",
    "        ], dtype=\"double\")\n",
    "\n",
    "        # Camera ná»™i táº¡i (giáº£ Ä‘á»‹nh Ä‘Æ¡n giáº£n)\n",
    "        focal_length = img_size[1]\n",
    "        center = (img_size[1]/2, img_size[0]/2)\n",
    "        camera_matrix = np.array([\n",
    "            [focal_length, 0, center[0]],\n",
    "            [0, focal_length, center[1]],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=\"double\")\n",
    "\n",
    "        # Giáº£ Ä‘á»‹nh khÃ´ng cÃ³ biáº¿n dáº¡ng tháº¥u kÃ­nh\n",
    "        dist_coeffs = np.zeros((4,1))\n",
    "\n",
    "        # Giáº£i bÃ i toÃ¡n PnP\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            # Chuyá»ƒn rotation vector sang rotation matrix\n",
    "            rmat, _ = cv2.Rodrigues(rotation_vector)\n",
    "            \n",
    "            # TÃ­nh Euler angles sá»­ dá»¥ng atan2 Ä‘á»ƒ trÃ¡nh gimbal lock vÃ  normalize tá»‘t hÆ¡n\n",
    "            # Pitch (Î¸): nghiÃªng dá»c\n",
    "            pitch = np.arctan2(-rmat[2,0], np.sqrt(rmat[0,0]**2 + rmat[1,0]**2)) * 180 / np.pi\n",
    "            \n",
    "            # Yaw (Ïˆ): xoay ngang\n",
    "            yaw = np.arctan2(rmat[1,0], rmat[0,0]) * 180 / np.pi\n",
    "            \n",
    "            # Roll (Ï†): nghiÃªng ngang\n",
    "            roll = np.arctan2(rmat[2,1], rmat[2,2]) * 180 / np.pi\n",
    "            \n",
    "            # Normalize cÃ¡c gÃ³c vá» [-180, 180] náº¿u cáº§n (trÃ¡nh wrap-around)\n",
    "            if pitch > 180:\n",
    "                pitch -= 360\n",
    "            elif pitch < -180:\n",
    "                pitch += 360\n",
    "            if yaw > 180:\n",
    "                yaw -= 360\n",
    "            elif yaw < -180:\n",
    "                yaw += 360\n",
    "            if roll > 180:\n",
    "                roll -= 360\n",
    "            elif roll < -180:\n",
    "                roll += 360\n",
    "            \n",
    "            # Váº½ landmarks (cÃ¡c Ä‘iá»ƒm má»‘c chÃ­nh) lÃªn áº£nh\n",
    "            for point in image_points:\n",
    "                cv2.circle(img, (int(point[0]), int(point[1])), 3, (0, 0, 255), -1)  # VÃ²ng trÃ²n Ä‘á»\n",
    "            \n",
    "            # Váº½ axes (trá»¥c pose) tá»« mÅ©i (nose tip)\n",
    "            nose_tip = (int(image_points[0][0]), int(image_points[0][1]))\n",
    "            \n",
    "            # Chiáº¿u cÃ¡c Ä‘iá»ƒm 3D axes lÃªn 2D (scale 100 Ä‘á»ƒ dá»… nhÃ¬n)\n",
    "            axis_points, _ = cv2.projectPoints(np.array([\n",
    "                [100.0, 0.0, 0.0],    # X-axis (red - yaw/right)\n",
    "                [0.0, 100.0, 0.0],    # Y-axis (green - pitch/up)\n",
    "                [0.0, 0.0, 100.0]     # Z-axis (blue - forward)\n",
    "            ]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "            \n",
    "            # Váº½ axes vá»›i mÃ u chuáº©n\n",
    "            p1 = nose_tip\n",
    "            # X-axis (red - yaw)\n",
    "            p2 = (int(axis_points[0][0][0]), int(axis_points[0][0][1]))\n",
    "            cv2.arrowedLine(img, p1, p2, (0, 0, 255), 2)  # Red\n",
    "            # Y-axis (green - pitch)\n",
    "            p2 = (int(axis_points[1][0][0]), int(axis_points[1][0][1]))\n",
    "            cv2.arrowedLine(img, p1, p2, (0, 255, 0), 2)  # Green\n",
    "            # Z-axis (blue - forward)\n",
    "            p2 = (int(axis_points[2][0][0]), int(axis_points[2][0][1]))\n",
    "            cv2.arrowedLine(img, p1, p2, (255, 0, 0), 2)  # Blue\n",
    "            \n",
    "            return pitch, yaw, roll, img\n",
    "        else:\n",
    "            return None, None, None, img\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in head pose estimation: {e}\")\n",
    "        return None, None, None, img\n",
    "\n",
    "# Widget cho head pose estimation\n",
    "uploader_pose = widgets.FileUpload(accept='image/*', multiple=False, description='Upload áº¢nh')\n",
    "pose_button = widgets.Button(description='Æ¯á»›c LÆ°á»£ng Head Pose', button_style='success')\n",
    "pose_output = widgets.Output()\n",
    "\n",
    "def on_pose_button_clicked(b):\n",
    "    with pose_output:\n",
    "        clear_output()\n",
    "        if not uploader_pose.value:\n",
    "            print(\"âš ï¸ Vui lÃ²ng upload áº£nh!\")\n",
    "            return\n",
    "        try:\n",
    "            file_info = uploader_pose.value[0]\n",
    "            image_bytes = file_info['content']\n",
    "            img = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "            if img is None:\n",
    "                print(\"âš ï¸ KhÃ´ng thá»ƒ Ä‘á»c áº£nh!\")\n",
    "                return\n",
    "            faces = model.get(img)\n",
    "            if len(faces) == 0:\n",
    "                print(\"âš ï¸ KhÃ´ng phÃ¡t hiá»‡n khuÃ´n máº·t trong áº£nh.\")\n",
    "                return\n",
    "            \n",
    "            # Láº¥y landmarks vÃ  tÃ­nh head pose + váº½ lÃªn áº£nh\n",
    "            landmarks = faces[0].landmark_3d_68  # Sá»­ dá»¥ng 3D landmarks 68 points\n",
    "            img_size = img.shape[:2]  # (height, width)\n",
    "            pitch, yaw, roll, visualized_img = estimate_head_pose(landmarks, img, img_size)\n",
    "            \n",
    "            # Chuyá»ƒn áº£nh visualized sang bytes Ä‘á»ƒ hiá»ƒn thá»‹\n",
    "            _, buffer = cv2.imencode('.png', visualized_img)\n",
    "            visualized_bytes = buffer.tobytes()\n",
    "            \n",
    "            # Hiá»ƒn thá»‹ áº£nh Ä‘Ã£ váº½\n",
    "            display(IPImage(data=visualized_bytes, width=300))\n",
    "            if pitch is not None:\n",
    "                print(f\"ðŸŽ¯ Head Pose:\")\n",
    "                print(f\"   Pitch (nghiÃªng dá»c): {pitch:.2f}Â°\")\n",
    "                print(f\"   Yaw (xoay ngang): {yaw:.2f}Â°\")\n",
    "                print(f\"   Roll (nghiÃªng ngang): {roll:.2f}Â°\")\n",
    "            else:\n",
    "                print(\"âš ï¸ KhÃ´ng thá»ƒ Æ°á»›c lÆ°á»£ng head pose.\")\n",
    "            \n",
    "            # Reset uploader\n",
    "            uploader_pose.value = ()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing image: {e}\")\n",
    "\n",
    "pose_button.on_click(on_pose_button_clicked)\n",
    "\n",
    "# Hiá»ƒn thá»‹ widget\n",
    "print(\"\\nðŸ§  Æ¯á»›c lÆ°á»£ng Head Pose tá»« áº£nh:\")\n",
    "display(widgets.VBox([uploader_pose, pose_button, pose_output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8497224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing InsightFace model (buffalo_l, CPU)...\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/hoangtrung/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "âœ… InsightFace model loaded successfully!\n",
      "\n",
      "ðŸ§  Æ¯á»›c lÆ°á»£ng Head Pose tá»« áº£nh:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5564a8f6814a6180d7587d2beebe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value=(), accept='image/*', description='Upload áº¢nh'), Button(button_style='success'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image as IPImage, clear_output\n",
    "import io\n",
    "import insightface\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Khá»Ÿi táº¡o InsightFace model\n",
    "try:\n",
    "    print(\"ðŸ”§ Initializing InsightFace model (buffalo_l, CPU)...\")\n",
    "    model = insightface.app.FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
    "    model.prepare(ctx_id=-1)  # CPU\n",
    "    print(\"âœ… InsightFace model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading InsightFace model: {e}\")\n",
    "    print(\"Please ensure 'insightface' and 'onnxruntime' are installed: `pip install insightface onnxruntime`\")\n",
    "    raise\n",
    "\n",
    "# Lá»›p PoseEstimator tá»« code tham kháº£o\n",
    "class PoseEstimator:\n",
    "    \"\"\"Estimate head pose according to the facial landmarks\"\"\"\n",
    "\n",
    "    def __init__(self, image_width, image_height):\n",
    "        \"\"\"Init a pose estimator.\n",
    "\n",
    "        Args:\n",
    "            image_width (int): input image width\n",
    "            image_height (int): input image height\n",
    "        \"\"\"\n",
    "        self.size = (image_height, image_width)\n",
    "        self.model_points_68 = self._get_full_model_points()\n",
    "\n",
    "        # Camera internals\n",
    "        self.focal_length = self.size[1]\n",
    "        self.camera_center = (self.size[1] / 2, self.size[0] / 2)\n",
    "        self.camera_matrix = np.array(\n",
    "            [[self.focal_length, 0, self.camera_center[0]],\n",
    "             [0, self.focal_length, self.camera_center[1]],\n",
    "             [0, 0, 1]], dtype=\"double\")\n",
    "\n",
    "        # Assuming no lens distortion\n",
    "        self.dist_coeefs = np.zeros((4, 1))\n",
    "\n",
    "        # Rotation vector and translation vector\n",
    "        self.r_vec = np.array([[0.01891013], [0.08560084], [-3.14392813]])\n",
    "        self.t_vec = np.array(\n",
    "            [[-14.97821226], [-10.62040383], [-2053.03596872]])\n",
    "\n",
    "    def _get_full_model_points(self):\n",
    "        \"\"\"Get all 68 3D model points (hardcoded from model.txt)\"\"\"\n",
    "        raw_value = [\n",
    "            -73.393523, -89.31522, 69.48,\n",
    "            -72.775015, -87.47549, 52.95,\n",
    "            -78.202742, -75.51543, 43.22,\n",
    "            -77.849368, -68.25236, 28.9,\n",
    "            -84.465767, -66.2853, 14.3,\n",
    "            -85.636795, -62.60433, -4.79,\n",
    "            -83.646607, -50.824226, -29.43,\n",
    "            -81.9452, -42.12885, -43.1,\n",
    "            -77.486112, -29.393612, -52.43,\n",
    "            -78.638795, -21.982149, -46.22,\n",
    "            -79.062126, -11.8819, -50.54,\n",
    "            -78.753606, -3.995317, -46.72,\n",
    "            -80.9039, 6.0479, -40.3,\n",
    "            -81.246819, 14.532797, -43.14,\n",
    "            -80.16907, 25.147775, -35.69,\n",
    "            -77.058449, 30.77057, -42.19,\n",
    "            -74.221957, 38.799888, -25.33,\n",
    "            0.0, 48.22903, -30.0,\n",
    "            72.775015, 87.47549, 52.95,\n",
    "            78.202742, 75.51543, 43.22,\n",
    "            77.849368, 68.25236, 28.9,\n",
    "            84.465767, 66.2853, 14.3,\n",
    "            85.636795, 62.60433, -4.79,\n",
    "            83.646607, 50.824226, -29.43,\n",
    "            81.9452, 42.12885, -43.1,\n",
    "            77.486112, 29.393612, -52.43,\n",
    "            78.638795, 21.982149, -46.22,\n",
    "            79.062126, 11.8819, -50.54,\n",
    "            78.753606, 3.995317, -46.72,\n",
    "            80.9039, -6.0479, -40.3,\n",
    "            81.246819, -14.532797, -43.14,\n",
    "            80.16907, -25.147775, -35.69,\n",
    "            77.058449, -30.77057, -42.19,\n",
    "            74.221957, -38.799888, -25.33,\n",
    "            0.0, -48.22903, -30.0,\n",
    "            36.845203, 65.246994, 47.58,\n",
    "            24.229742, 55.074575, 56.61,\n",
    "            11.971302, 50.004105, 61.5,\n",
    "            0.0, 45.0, 66.0,\n",
    "            -11.971302, 50.004105, 61.5,\n",
    "            -24.229742, 55.074575, 56.61,\n",
    "            -36.845203, 65.246994, 47.58,\n",
    "            29.680221, 20.650688, 66.49,\n",
    "            18.775135, 13.113989, 70.63,\n",
    "            9.226971, 6.391162, 75.14,\n",
    "            0.0, 0.0, 80.0,\n",
    "            -9.226971, 6.391162, 75.14,\n",
    "            -18.775135, 13.113989, 70.63,\n",
    "            -29.680221, 20.650688, 66.49,\n",
    "            37.389622, -3.124168, 55.5,\n",
    "            24.474024, -6.189816, 59.76,\n",
    "            12.229014, -3.160895, 66.25,\n",
    "            0.0, 0.0, 70.0,\n",
    "            -12.229014, -3.160895, 66.25,\n",
    "            -24.474024, -6.189816, 59.76,\n",
    "            -37.389622, -3.124168, 55.5,\n",
    "            22.755451, -18.931895, 45.9,\n",
    "            14.2635, -23.070902, 55.13,\n",
    "            0.0, -27.0, 60.0,\n",
    "            -14.2635, -23.070902, 55.13,\n",
    "            -22.755451, -18.931895, 45.9,\n",
    "            12.482902, -39.47496, 37.17,\n",
    "            3.733898, -41.321726, 46.31,\n",
    "            0.0, -43.0, 51.0,\n",
    "            -3.733898, -41.321726, 46.31,\n",
    "            -12.482902, -39.47496, 37.17,\n",
    "            0.0, -73.0, 15.0,\n",
    "            0.0, -85.0, 0.0,\n",
    "            0.0, -77.0, -37.0,\n",
    "            0.0, -63.0, -65.0,\n",
    "            0.0, -48.0, -84.0,\n",
    "            0.0, -28.0, -96.0,\n",
    "            0.0, -8.0, -102.0\n",
    "        ]\n",
    "        model_points = np.array(raw_value, dtype=np.float32)\n",
    "        model_points = np.reshape(model_points, (3, -1)).T\n",
    "\n",
    "        # Transform the model into a front view.\n",
    "        model_points[:, 2] *= -1\n",
    "\n",
    "        return model_points\n",
    "\n",
    "    def solve(self, points):\n",
    "        \"\"\"Solve pose with all the 68 image points\n",
    "        Args:\n",
    "            points (np.ndarray): points on image.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: (rotation_vector, translation_vector) as pose.\n",
    "        \"\"\"\n",
    "        if self.r_vec is None:\n",
    "            (_, rotation_vector, translation_vector) = cv2.solvePnP(\n",
    "                self.model_points_68, points, self.camera_matrix, self.dist_coeefs)\n",
    "            self.r_vec = rotation_vector\n",
    "            self.t_vec = translation_vector\n",
    "\n",
    "        (_, rotation_vector, translation_vector) = cv2.solvePnP(\n",
    "            self.model_points_68,\n",
    "            points,\n",
    "            self.camera_matrix,\n",
    "            self.dist_coeefs,\n",
    "            rvec=self.r_vec,\n",
    "            tvec=self.t_vec,\n",
    "            useExtrinsicGuess=True)\n",
    "\n",
    "        return (rotation_vector, translation_vector)\n",
    "\n",
    "    def visualize(self, image, pose, color=(255, 255, 255), line_width=2):\n",
    "        \"\"\"Draw a 3D box as annotation of pose\"\"\"\n",
    "        rotation_vector, translation_vector = pose\n",
    "        point_3d = []\n",
    "        rear_size = 75\n",
    "        rear_depth = 0\n",
    "        point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "        point_3d.append((-rear_size, rear_size, rear_depth))\n",
    "        point_3d.append((rear_size, rear_size, rear_depth))\n",
    "        point_3d.append((rear_size, -rear_size, rear_depth))\n",
    "        point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "\n",
    "        front_size = 100\n",
    "        front_depth = 100\n",
    "        point_3d.append((-front_size, -front_size, front_depth))\n",
    "        point_3d.append((-front_size, front_size, front_depth))\n",
    "        point_3d.append((front_size, front_size, front_depth))\n",
    "        point_3d.append((front_size, -front_size, front_depth))\n",
    "        point_3d.append((-front_size, -front_size, front_depth))\n",
    "        point_3d = np.array(point_3d, dtype=np.float32).reshape(-1, 3)\n",
    "\n",
    "        # Map to 2d image points\n",
    "        (point_2d, _) = cv2.projectPoints(point_3d,\n",
    "                                          rotation_vector,\n",
    "                                          translation_vector,\n",
    "                                          self.camera_matrix,\n",
    "                                          self.dist_coeefs)\n",
    "        point_2d = np.int32(point_2d.reshape(-1, 2))\n",
    "\n",
    "        # Draw all the lines\n",
    "        cv2.polylines(image, [point_2d], True, color, line_width, cv2.LINE_AA)\n",
    "        cv2.line(image, tuple(point_2d[1]), tuple(point_2d[6]), color, line_width, cv2.LINE_AA)\n",
    "        cv2.line(image, tuple(point_2d[2]), tuple(point_2d[7]), color, line_width, cv2.LINE_AA)\n",
    "        cv2.line(image, tuple(point_2d[3]), tuple(point_2d[8]), color, line_width, cv2.LINE_AA)\n",
    "\n",
    "    def draw_axes(self, img, pose):\n",
    "        R, t = pose\n",
    "        img = cv2.drawFrameAxes(img, self.camera_matrix, self.dist_coeefs, R, t, 30)\n",
    "\n",
    "# Widget cho head pose estimation\n",
    "uploader_pose = widgets.FileUpload(accept='image/*', multiple=False, description='Upload áº¢nh')\n",
    "pose_button = widgets.Button(description='Æ¯á»›c LÆ°á»£ng Head Pose', button_style='success')\n",
    "pose_output = widgets.Output()\n",
    "\n",
    "def on_pose_button_clicked(b):\n",
    "    with pose_output:\n",
    "        clear_output()\n",
    "        if not uploader_pose.value:\n",
    "            print(\"âš ï¸ Vui lÃ²ng upload áº£nh!\")\n",
    "            return\n",
    "        try:\n",
    "            file_info = uploader_pose.value[0]\n",
    "            image_bytes = file_info['content']\n",
    "            img = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "            if img is None:\n",
    "                print(\"âš ï¸ KhÃ´ng thá»ƒ Ä‘á»c áº£nh!\")\n",
    "                return\n",
    "            faces = model.get(img)\n",
    "            if len(faces) == 0:\n",
    "                print(\"âš ï¸ KhÃ´ng phÃ¡t hiá»‡n khuÃ´n máº·t trong áº£nh.\")\n",
    "                return\n",
    "            \n",
    "            # Láº¥y landmarks (InsightFace tráº£ vá» 2D or 3D, nhÆ°ng code dÃ¹ng 2D)\n",
    "            landmarks = faces[0].landmark_3d_68[:, :2]  # Láº¥y x,y cho 68 points\n",
    "            \n",
    "            # Táº¡o PoseEstimator vá»›i size áº£nh\n",
    "            height, width = img.shape[:2]\n",
    "            pose_estimator = PoseEstimator(width, height)\n",
    "            \n",
    "            # TÃ­nh pose\n",
    "            rotation_vector, translation_vector = pose_estimator.solve(landmarks)\n",
    "            \n",
    "            # Váº½ visualization\n",
    "            visualized_img = img.copy()\n",
    "            pose_estimator.visualize(visualized_img, (rotation_vector, translation_vector))\n",
    "            pose_estimator.draw_axes(visualized_img, (rotation_vector, translation_vector))\n",
    "            \n",
    "            # TÃ­nh Euler angles tá»« rotation_vector\n",
    "            rmat, _ = cv2.Rodrigues(rotation_vector)\n",
    "            pitch = np.arctan2(-rmat[2,0], np.sqrt(rmat[0,0]**2 + rmat[1,0]**2)) * 180 / np.pi\n",
    "            yaw = np.arctan2(rmat[1,0], rmat[0,0]) * 180 / np.pi\n",
    "            roll = np.arctan2(rmat[2,1], rmat[2,2]) * 180 / np.pi\n",
    "            \n",
    "            # Normalize gÃ³c vá» [-180, 180]\n",
    "            for angle in [pitch, yaw, roll]:\n",
    "                if angle > 180:\n",
    "                    angle -= 360\n",
    "                elif angle < -180:\n",
    "                    angle += 360\n",
    "            \n",
    "            # Chuyá»ƒn áº£nh visualized sang bytes\n",
    "            _, buffer = cv2.imencode('.png', visualized_img)\n",
    "            visualized_bytes = buffer.tobytes()\n",
    "            \n",
    "            # Hiá»ƒn thá»‹\n",
    "            display(IPImage(data=visualized_bytes, width=300))\n",
    "            print(f\"ðŸŽ¯ Head Pose:\")\n",
    "            print(f\"   Pitch (nghiÃªng dá»c): {pitch:.2f}Â°\")\n",
    "            print(f\"   Yaw (xoay ngang): {yaw:.2f}Â°\")\n",
    "            print(f\"   Roll (nghiÃªng ngang): {roll:.2f}Â°\")\n",
    "            \n",
    "            # Reset uploader\n",
    "            uploader_pose.value = ()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing image: {e}\")\n",
    "\n",
    "pose_button.on_click(on_pose_button_clicked)\n",
    "\n",
    "# Hiá»ƒn thá»‹ widget\n",
    "print(\"\\nðŸ§  Æ¯á»›c lÆ°á»£ng Head Pose tá»« áº£nh:\")\n",
    "display(widgets.VBox([uploader_pose, pose_button, pose_output]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
